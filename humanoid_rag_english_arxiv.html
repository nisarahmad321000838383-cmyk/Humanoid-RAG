<!DOCTYPE html>
<html lang="en" dir="ltr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>humanoid RAG: A Discourse-Aware Hierarchical Retrieval-Augmented Generation (RAG) System with Multi-Level Question Planning</title>
    <link rel="stylesheet" href="style.css">
    <style>
        /* arXiv-like style with LTR direction */
        body {
            direction: ltr;
            text-align: left;
            font-family: 'Times New Roman', 'Traditional Arabic', serif;
            max-width: 650px;
            margin: 2em auto;
            padding: 0 1em;
            line-height: 1.6;
            background: #ffffff;
            color: #000000;
            font-size: 11pt;
            position: relative;
        }
        
        /* Paper header - arXiv style */
        .paper-header {
            text-align: center;
            margin-bottom: 2em;
            padding-bottom: 1em;
        }
        .paper-title {
            font-size: 17pt;
            font-weight: bold;
            margin-bottom: 1.5em;
            line-height: 1.3;
        }
        .author-info {
            font-size: 12pt;
            margin: 0.5em 0;
        }
        .affiliation {
            font-size: 10pt;
            color: #000;
            font-style: italic;
            margin: 0.3em 0;
        }
        
        /* Abstract - arXiv style */
        .abstract-section {
            margin: 2em 0;
            padding: 0;
            background: transparent;
            border: none;
        }
        .abstract-title {
            font-weight: bold;
            font-size: 12pt;
            text-align: center;
            margin-bottom: 0.5em;
        }
        .abstract-section p {
            text-align: justify;
            text-justify: inter-word;
            margin: 0.8em 0;
        }
        
        /* Sections - arXiv style */
        .section {
            margin: 1.5em 0;
        }
        .section-title {
            font-size: 13pt;
            font-weight: bold;
            margin: 1.2em 0 0.8em 0;
        }
        .subsection-title {
            font-size: 12pt;
            font-weight: bold;
            margin: 1em 0 0.6em 0;
            font-style: italic;
        }
        .subsubsection-title {
            font-size: 11pt;
            font-weight: bold;
            margin: 0.8em 0 0.5em 0;
        }
        
        /* Paragraphs */
        p {
            text-align: justify;
            text-justify: inter-word;
            margin: 0.8em 0;
            text-indent: 0;
        }
        /* Figures and images */
        .figure {
            text-align: center;
            margin: 1em 0;
            padding: 0;
            background: transparent;
            border: none;
        }
        .figure-caption {
            margin-top: 0.5em;
            font-style: italic;
            font-size: 10pt;
        }
        
        /* Tables - arXiv style */
        .table {
            width: 100%;
            margin: 1em auto;
            border-collapse: collapse;
        }
        .table th, .table td {
            border: 1px solid #000;
            padding: 0.3em 0.5em;
            text-align: center;
            font-size: 10pt;
        }
        .table th {
            background: transparent;
            font-weight: bold;
            border-bottom: 2px solid #000;
        }
        table {
            margin: 1em auto;
            border-collapse: collapse;
        }
        table th, table td {
            border: 1px solid #000;
            padding: 0.3em 0.5em;
            font-size: 10pt;
        }
        table th {
            font-weight: bold;
            border-bottom: 2px solid #000;
        }
        
        /* Equations */
        .equation {
            text-align: center;
            margin: 1em 0;
            font-style: italic;
        }
        
        /* Algorithms */
        .algorithm {
            margin: 1em 0;
            padding: 0.5em;
            border: 1px solid #000;
            background: transparent;
            font-family: 'Courier New', monospace;
            font-size: 9pt;
            direction: ltr;
            text-align: left;
        }
        
        /* Definitions and examples */
        .definition {
            margin: 1em 0;
            padding: 0.5em 1em;
            border-left: 3px solid #000;
            background: transparent;
        }
        
        /* Warnings */
        .warning {
            margin: 1em 0;
            padding: 0.5em 1em;
            border: 2px solid #000;
            background: transparent;
            font-weight: bold;
        }
        
        /* References */
        .references {
            margin-top: 2em;
        }
        .reference-item {
            margin: 0.5em 0;
            padding-left: 2em;
            text-indent: -2em;
            font-size: 9pt;
            text-align: left;
        }
        
        /* Lists */
        ul, ol {
            margin: 0.5em 0;
            padding-left: 2em;
        }
        li {
            margin: 0.3em 0;
        }
        
        /* Code */
        code {
            font-family: 'Courier New', monospace;
            font-size: 9pt;
            direction: ltr;
            display: inline;
            background: transparent;
        }
        pre {
            font-family: 'Courier New', monospace;
            font-size: 9pt;
            margin: 1em 0;
            padding: 0.5em;
            overflow-x: auto;
            direction: ltr;
            text-align: left;
            background: transparent;
            border: 1px solid #ccc;
        }
        
        /* Additional styles to better resemble arXiv */
        strong {
            font-weight: bold;
        }
        em {
            font-style: italic;
        }
        h1, h2, h3, h4 {
            font-weight: bold;
            line-height: 1.3;
        }

        /* Print-only page number */
        .print-page-number {
            display: none;
        }
        
        /* Page numbers - arXiv style */
        @media print {
            body {
                margin: 0;
                padding: 0;
            }
            
            @page {
                size: A4;
                margin: 20mm 18mm 20mm 18mm;
            }
            
            /* Page number with JavaScript on each printed page */
            .print-page-number {
                display: block;
                position: absolute;
                left: 50%;
                transform: translateX(-50%);
                font-size: 10pt;
                text-align: center;
                white-space: nowrap;
                background: #fff;
                padding: 0 6px;
                z-index: 9999;
                pointer-events: none;
            }
        }
        
        /* Show page number in web only - hidden in print */
        .page-number {
            position: fixed;
            bottom: 20px;
            left: 50%;
            transform: translateX(-50%);
            font-size: 10pt;
            color: #666;
            background: white;
            padding: 5px 15px;
            border: 1px solid #ddd;
            border-radius: 3px;
            z-index: 1000;
        }
        
        /* Header with arXiv identifier - hidden in print */
        .arxiv-header {
            position: fixed;
            top: 10px;
            left: 10px;
            font-size: 9pt;
            color: #666;
            background: white;
            padding: 5px;
            border: 1px solid #ddd;
            z-index: 1000;
        }
        
        /* Hide labels in print */
        @media print {
            .page-number {
                display: none !important;
            }
            .arxiv-header {
                display: none !important;
            }
        }
    </style>
</head>
<body>
    <!-- Page number -->
    <div class="page-number" id="pageNumber">Page 1</div>
    
    <div class="paper-header">
        <div class="paper-title">
            humanoid RAG: A Discourse-Aware Hierarchical Retrieval-Augmented Generation (RAG) System<br>
            with Multi-Level Question Planning
        </div>
        <div class="author-info">
            Fardin  Ibrahimi
        </div>
        <div class="affiliation">
            B.Sc. in Computer Science
        </div>
        <div class="affiliation">
            Independent Research
        </div>
        <div class="affiliation" style="margin-top: 15px; font-size: 0.9em;">
            February 2026
        </div>
    </div>

    <div class="abstract-section">
        <div class="abstract-title">Abstract</div>
        <p>
            Retrieval-Augmented Generation (RAG) systems face fundamental limitations in answering complex questions that require multi-step reasoning.
            This paper introduces humanoid RAG: a novel framework that combines hierarchical question planning with discourse modeling. Our system has two levels:
            (1) a high-level planner that decomposes complex questions, verifies information sufficiency, and generates supplemental questions, and (2) low-level searchers that perform
            hybrid retrieval (sparse, dense, and web). Moreover, we apply discourse modeling at two levels: rhetorical structure trees (RST) at the section level to identify nucleus information,
            and an inter-section rhetorical graph to model relations such as contradiction and elaboration. This discourse structure guides response planning and ensures coherent and accurate generation.
        </p>
        <p>
            <strong>Important Note:</strong> This research has only been tested with the LLaMA 7B model on private data.
            No evaluation has been conducted on public benchmark datasets.
        </p>
        <p>
            <strong>Keywords:</strong> Retrieval-Augmented Generation (RAG), discourse modeling, hierarchical planning, question answering, multi-step reasoning
        </p>
    </div>

    <div class="section">
        <div class="section-title">1. Introduction</div>
        
        <p>
            Large language models (LLMs) have shown remarkable capabilities in text generation, but they suffer from fundamental limitations: outdated parametric knowledge, hallucination,
            and lack of access to domain-specific information. Retrieval-Augmented Generation (RAG) systems mitigate these limitations by combining external document retrieval and LLM-based generation.
            However, existing RAG systems perform poorly on complex questions that require multi-step reasoning, integration of conflicting information, or decisions about information sufficiency.
        </p>

        <div class="subsection-title">1.1 Motivation</div>
        <p>
            Real-world questions often require decomposition into subproblems, iterative retrieval, and careful integration of information. For example, a question like "Are Android and iOS both mobile OS?"
            requires identifying two sub-questions, retrieving information about each OS, and then comparing them. Traditional single-step RAG systems fail at such tasks.
        </p>
        <p>
            In addition, retrieved documents may contain contradictions (e.g., old vs. new information), provide incomplete information, or include low-importance sentences that mislead the LLM.
            Understanding discourse relations within and across documents is essential for generating accurate and coherent answers.
        </p>

        <div class="subsection-title">1.2 Main Contributions</div>
        <p>We present humanoid RAG, a novel framework that:</p>
        <ol>
            <li><strong>Hierarchical Planning:</strong> a two-level architecture with a high-level planner for question decomposition, sufficiency verification, and information supplementation, and low-level searchers for hybrid retrieval (sparse, dense, web).</li>
            <li><strong>Two-Level Discourse Modeling:</strong> intra-section RST trees to identify nucleus information, and inter-section rhetorical graphs to model relations such as contradiction and elaboration.</li>
            <li><strong>Discourse-Driven Planning:</strong> response plan generation based on discourse structure to ensure coherence and accuracy.</li>
        </ol>

        <div class="subsection-title">1.3 Paper Organization</div>
        <p>
            The rest of this paper is organized as follows: Section 2 reviews related work.
            Section 3 explains the methodology. Section 4 presents architectural details.
            Section 5 provides a theoretical analysis. Section 6 discusses limitations.
            Section 7 concludes.
        </p>
    </div>

    <div class="section">
        <div class="section-title">2. Related Work</div>

        <div class="subsection-title">2.1 Retrieval-Augmented Generation (RAG) Systems</div>
        <p>
            RAG systems improve the accuracy and attribution of LLMs by combining document retrieval and text generation.
            Early works such as the original RAG and later works like Self-RAG and FLARE have introduced improvements in retrieval and generation.
            However, these approaches mainly focus on single-step questions and are weak in multi-step reasoning.
        </p>

        <div class="subsection-title">2.2 Multi-Step Reasoning</div>
        <p>
            Methods such as IRCoT and MindSearch extend RAG for multi-step reasoning by decomposing complex questions into sub-questions and performing iterative retrieval.
            Our work goes beyond these approaches by introducing hierarchical planning and discourse modeling.
        </p>

        <div class="subsection-title">2.3 Discourse Modeling</div>
        <p>
            Rhetorical Structure Theory (RST) is a simple way to look at how sentences connect. It asks: which sentence is the main point, and which sentences support it?
            Think of a short text like a tree: the trunk is the main idea, and the branches are extra details.
            <br><br>
            <strong>Simple example:</strong> “The battery is low. So the phone will shut down soon.” The first sentence is the main point; the second explains why.
            <br><br>
            Recent works use RST to improve summarization and text generation because it helps models focus on the most important sentence instead of noise.
            We apply RST in RAG to identify the nucleus (core) information and then combine documents more cleanly, so the answer is clearer and less confused.
        </p>

        <div class="subsection-title">2.4 Foundational Works</div>
        <p>
            This research is based on two main arXiv papers. In very simple terms, each paper solved one big problem:
        </p>
        <ol>
            <li><strong>LevelRAG:</strong> focuses on hierarchy. It breaks a hard question into smaller questions and answers them step by step. Think of it like solving a big task by splitting it into mini‑tasks.</li>
            <li><strong>DiscoRAG:</strong> focuses on discourse. It looks at how sentences connect (cause, contrast, background) so the final answer is clear and not mixed up.</li>
        </ol>
        <p>
            <strong>Why we mention them:</strong> LevelRAG teaches us how to plan in levels, and DiscoRAG teaches us how to organize the information. humanoid RAG combines both ideas, so it can plan the steps and also explain the answer clearly.
        </p>
    </div>

    <div class="section">
        <div class="section-title">3. Methodology</div>

        <div class="subsection-title">3.1 Problem Definition</div>
        <p>
            Consider a question <em>q</em> and a set of documents <em>D</em>. The goal is to produce an answer
            <em>a</em> that accurately responds to <em>q</em> and is grounded in information retrieved from <em>D</em>.
            For multi-step questions, <em>q</em> should be decomposed into sub-questions
            {<em>q</em>₁, <em>q</em>₂, ..., <em>q</em>ₙ} each requiring separate retrieval and processing.
        </p>

        <div class="subsection-title">3.2 Architecture Overview</div>
        <p>
            humanoid RAG consists of a two-level architecture:
        </p>
        <ul>
            <li><strong>High level (planner):</strong> the “manager.” It splits a big question into smaller ones, makes short summaries, checks if the info is enough, and asks for more if needed.</li>
            <li><strong>Low level (searchers):</strong> the “helpers.” They search in three ways at the same time: sparse = keyword search, dense = meaning search, web = internet search.</li>
        </ul>
        <p>
            <strong>Simple example:</strong> If the question is “Is this laptop good for video calls?”, the planner breaks it into parts (camera, microphone, internet).
            Then the searchers look for each part using keywords, meaning, and the web. After that, the planner checks if the answer is complete.
        </p>

        <div class="figure">
            <pre style="text-align: center; font-family: monospace;">
┌──────────────────────────────────────────────┐
│                  User Query                  │
└────────────────┬─────────────────────────────┘
                 ▼
┌────────────────────────────────────────────────┐
│             High-Level Planner                 │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐     │
│  │Decompose │→│ Summarize │→│  Verify  │     │
│  └──────────┘  └──────────┘  └──────────┘     │
│                      ↓                         │
│               ┌──────────┐                     │
│               │Supplement│                     │
│               └──────────┘                     │
└────┬────────────┬────────────┬────────────────┘
     │            │            │
┌────▼────┐  ┌───▼────┐  ┌────▼────┐
│Searcher │  │Searcher│  │Searcher│
│ sparse  │  │ dense  │  │  web   │
└────┬────┘  └───┬────┘  └────┬────┘
     │            │            │
     └────────────┼────────────┘
                  ▼
         ┌──────────────────┐
         │ Discourse Modeling│
         └──────────────────┘
                  ▼
         ┌──────────────────┐
         │ Response Generation│
         └──────────────────┘
            </pre>
            <div class="figure-caption">Figure 1: Architecture overview of humanoid RAG</div>
        </div>

        <div class="subsection-title">3.3 Workflow</div>
        <p>The humanoid RAG workflow includes the following steps:</p>
        <ol>
            <li><strong>Decomposition (Decompose):</strong> break the big question into smaller questions (like a checklist).</li>
            <li><strong>Retrieval (Retrieve):</strong> for each small question, search in three ways at the same time: keywords, meaning, and the web.</li>
            <li><strong>Discourse Modeling (Discourse Modeling):</strong> figure out how sentences relate (main point, support, contrast).</li>
            <li><strong>Summarization (Summarize):</strong> keep only the most important facts from each part.</li>
            <li><strong>Verification (Verify):</strong> check if we have enough facts to answer the original question.</li>
            <li><strong>Supplementation (Supplement):</strong> if something is missing, ask one more small question and search again.</li>
            <li><strong>Planning (Plan):</strong> decide the best order to explain the answer.</li>
            <li><strong>Generation (Generate):</strong> write the final answer clearly.</li>
        </ol>
    </div>

    <div class="section">
        <div class="section-title">4. Architecture Details</div>

        <div class="subsection-title">4.1 High-Level Planner</div>
        
        <div class="subsubsection-title">4.1.1 Decomposition Operation</div>
        <p>
            The planner uses an LLM to decompose a complex question <em>q</em> into a set of atomic sub-questions
            {<em>q</em>₁, <em>q</em>₂, ..., <em>q</em>ₙ}. This operation is performed with a prompt designed to ask the LLM
            to split the question into simpler questions.
        </p>
        
        <div class="definition">
            <strong>Decomposition Example:</strong>
            <p><strong>Question:</strong> "Can both smartphone and laptop connect to Wi-Fi?"</p>
            <p><strong>Sub-questions:</strong></p>
            <ul>
                <li>q₁: "Can a smartphone connect to Wi-Fi?"</li>
                <li>q₂: "Can a laptop connect to Wi-Fi?"</li>
                <li>q₃: "Do both devices support Wi-Fi connectivity?"</li>
            </ul>
        </div>

        <div class="subsubsection-title">4.1.2 Summarization Operation</div>
        <p>
            For each sub-question <em>qᵢ</em> and its retrieved passages, the planner produces a short summary
            <em>sᵢ</em> that keeps only the most important facts. Think of it as “short notes” made from longer text.
            <br><br>
            <strong>Simple example:</strong> If the passages say “The laptop has a 1080p camera” and “The microphone is built‑in,” the summary keeps just those two key points.
            <br><br>
            This operation uses RST trees to prioritize nucleus sentences (the main points) and drop extra details.
            <br><br>
            <strong>Is this only possible with a prompt?</strong> A prompt helps, but not enough on its own. We also do basic preprocessing:
            we split text into chunks, detect the nucleus/satellite with RST, and then ask the LLM to summarize only the nucleus parts.
            So it is “prompt + structure,” not just a prompt. Here, “structure” means the organized labels we add (which sentences are nucleus vs. satellite and how chunks relate), so the LLM knows what to keep and what to ignore.
        </p>
        <p>
            <strong>How do we do RST in Python (simple):</strong> There is no single built‑in Python library for RST parsing.
            In practice, people use a pretrained RST parser from research code and call it from Python to label
            sentences as nucleus/satellite. There is no “one best” parser for everyone, but some of the most commonly used open‑source options in research are:
            <strong>RSTFinder</strong> (ETS) (https://github.com/EducationalTestingService/rstfinder),
            <strong>DPLP</strong> (https://github.com/jiyfeng/DPLP), and
            <strong>IsaNLP RST</strong> (https://pypi.org/project/isanlp-rst/).
            RSTFinder and DPLP are on GitHub, and IsaNLP RST is available on PyPI (so you can install it with <code>pip</code>).
            If you don’t have a parser yet, a very simple fallback is to keep the first or most informative sentence as “nucleus.”
        </p>
        <div class="definition">
            <strong>Simple Python Sketch (with comments):</strong>
            <pre style="direction: ltr; text-align: left;">
# Example text chunks we retrieved for a sub-question
chunks = [
    "The laptop has a 1080p camera. It is light.",
    "The microphone is built-in. The color is silver."
]

# Combine chunks into one text (most RST parsers expect full text)
text = " ".join(chunks)

# Run IsaNLP RST parser (simple idea; API names may vary by version)
# rst_tree = isanlp_rst.parse(text)

# From the RST tree, keep only nucleus sentences (main points)
# nucleus = [s.text for s in rst_tree.sentences if s.role == "nucleus"]

# Fallback if parser is not available (keep the most informative sentences)
nucleus = [
    "The laptop has a 1080p camera",
    "The microphone is built-in"
]

# Join nucleus sentences into a short summary
summary = ". ".join(nucleus) + "."

# Show the final short summary
print(summary)
            </pre>
            <strong>Expected output:</strong> The laptop has a 1080p camera. The microphone is built-in.
        </div>

        <div class="subsubsection-title">4.1.3 Verification Operation</div>
        <p>
            The planner checks whether the collected summaries {<em>s</em>₁, ..., <em>sₙ</em>} are sufficient
            to answer the main question <em>q</em>. If information is insufficient, the system moves to the
            supplementation operation.
        </p>

        <div class="subsubsection-title">4.1.4 Supplementation Operation</div>
        <p>
            If information is insufficient, the planner generates new supplemental questions and repeats the
            retrieval-summarization-verification cycle. This process continues until information is sufficient.
        </p>

        <div class="subsection-title">4.2 Low-Level Searchers</div>

        <div class="subsubsection-title">4.2.1 Sparse Searcher</div>
        <p>
            The sparse searcher uses the BM25 algorithm for keyword-based retrieval.
            It includes three feedback operations:
        </p>
        <ul>
            <li><strong>Extend:</strong> add related keywords to increase coverage</li>
            <li><strong>Filter:</strong> remove irrelevant phrases with the NOT operator</li>
            <li><strong>Emphasize:</strong> increase the weight of key keywords</li>
        </ul>

        <div class="definition">
            <strong>Sparse Query Example:</strong>
            <pre style="direction: ltr; text-align: left;">
Initial Query: "Wi-Fi" connection
↓ (if results are weak)
Extended: "Wi-Fi" connection "wireless"
↓ (if still weak)
Filtered: "Wi-Fi" connection -Bluetooth -cable
↓ (if needed)
Emphasized: "Wi-Fi"^2.0 connection
            </pre>
        </div>

        <div class="subsubsection-title">4.2.2 Dense Searcher</div>
        <p>
            The dense searcher uses embedding models for semantic retrieval.
            In this work, we perform dense search using transformer-based embedding models.
            To improve retrieval, we use the HyDE (Hypothetical Document Embeddings) technique:
            the LLM generates a hypothetical document that contains the answer, then the embedding of that
            document is used to retrieve similar real documents.
        </p>
        <p>
            <strong>Simple example:</strong> Question: “Which phone has fast battery charging?” The LLM first writes a short fake answer like
            “This phone supports fast charging and fills the battery in about 30 minutes.” We embed that fake answer and use it to find real documents
            that talk about fast charging, even if they don’t use the exact same words (because dense search matches meaning, not just keywords).
        </p>

        <div class="subsubsection-title">4.2.3 Web Searcher</div>
        <p>
            The web searcher uses web search APIs to access current and broad information.
            In this paper, we use a free API: the DuckDuckGo Instant Answer API.
            This searcher is especially useful for questions that require up-to-date information or information
            outside the local dataset.
        </p>
        <p>
            <strong>Limitations (DuckDuckGo, simple):</strong> It often returns short answers instead of full results, coverage can be limited for niche topics, and some queries return no answer.
        </p>

        <div class="subsection-title">4.3 Discourse Modeling</div>

        <div class="subsubsection-title">4.3.1 Intra-Section RST Trees</div>
        <p>
            For each retrieved passage, we build a Rhetorical Structure Theory (RST) tree (called a “tree” because it shows a hierarchy: main point at the top, supporting sentences below) that represents its
            internal discourse structure. The RST tree splits sentences into nucleus and satellite:
        </p>
        <ul>
            <li><strong>Nucleus:</strong> sentences containing core, essential information</li>
            <li><strong>Satellite:</strong> sentences containing supporting or auxiliary information</li>
        </ul>

        <div class="definition">
            <strong>RST Tree Example:</strong>
            <p>Text: "Smartphones run on batteries. [Sentence 1] A charger is used to refill the battery. [Sentence 2]
            But without power, a charger does not work. [Sentence 3] That is why a power bank is useful when traveling.
            [Sentence 4]"</p>
            <ul>
                <li>Nucleus: Sentence 3 (main information)</li>
                <li>Satellite: Sentences 1, 2, 4 (background and consequence)</li>
            </ul>
        </div>

        <div class="subsubsection-title">4.3.2 Inter-Section Rhetorical Graph</div>
        <p>
            To model relationships between different sections, we build a rhetorical graph <em>G</em> = (<em>V</em>, <em>E</em>)
            where each node <em>v</em> ∈ <em>V</em> represents a section and each edge <em>e</em> ∈ <em>E</em> represents a rhetorical relation.
            The main relations are:
        </p>
        <p>
            <strong>Simple meaning:</strong> <em>G</em> is the whole map, <em>V</em> is the list of sections (each one is a small <em>v</em>), and <em>E</em> is the list of relations between them (each one is a small <em>e</em>).
        </p>
        <ul>
            <li><strong>CONTRADICTS:</strong> sections contain conflicting information (e.g., one says “This cable transfers data,” another says “This cable is charge‑only.”)</li>
            <li><strong>ELABORATES:</strong> one section elaborates another (e.g., one says “The laptop is fast,” another lists the CPU and RAM details.)</li>
            <li><strong>BACKGROUND_FOR:</strong> one section provides background for another (e.g., one says “This model was released in 2018,” another explains its current performance.)</li>
            <li><strong>CAUSES:</strong> one section is the cause of another (e.g., “The battery is old” → “The phone shuts down quickly.”)</li>
        </ul>

        <div class="definition">
            <strong>Rhetorical Graph Example:</strong>
            <p>Question: "Do all USB cables support both charging and data transfer?"</p>
            <ul>
                <li>Section 1 (general claim): "All USB cables support both charging and data transfer"</li>
                <li>Section 2 (exception): "Some USB cables are made only for charging"</li>
                <li>Section 3 (additional explanation): "High-quality USB cables usually support both charging and data"</li>
            </ul>
            <p>Relations:</p>
            <ul>
                <li>Section 1 → Section 2: CONTRADICTS</li>
                <li>Section 3 → Section 1: ELABORATES</li>
                <li>Section 2 → Section 3: BACKGROUND_FOR</li>
            </ul>
        </div>

        <div class="subsubsection-title">4.3.3 Discourse-Based Planning</div>
        <p>
            Based on the rhetorical graph, the system creates a plan for response generation. This plan specifies
            the order of information, how to handle contradictions, and appropriate transition words.
        </p>

        <div class="algorithm">
            <strong>Algorithm 1: Discourse-Based Planning</strong>
            <pre>
Input: rhetorical graph G = (V, E)
Output: response plan P

1: Initialize plan P = []
2: Identify contradictions in G
3: For each contradiction:
4:    Add historical context (BACKGROUND) to P
5:    Add contrast marker ("but", "however") to P
6:    Add current information to P
7: Identify elaborations in G
8: For each elaboration:
9:    Add main claim to P
10:   Add elaborating details to P
11: Return P
            </pre>
        </div>
    </div>

    <div class="section">
        <div class="section-title">5. Theoretical Analysis and Expected Benefits</div>

        <div class="warning">
            <strong>⚠️ Important Notice:</strong>
            <p>
                This section does not include empirical results from benchmark datasets. This research has only been tested on private data.
                The content below describes the theoretical framework and expected benefits based on system design, not empirical results from scientific evaluation.
            </p>
        </div>

        <div class="subsection-title">5.1 Theoretical Advantages of the Framework</div>
        
        <div class="subsubsection-title">5.1.1 Multi-Step Reasoning</div>
        <p>
            The hierarchical architecture of humanoid RAG is designed to handle multi-step questions:
        </p>
        <ul>
            <li><strong>Question decomposition:</strong> complex questions are decomposed into atomic sub-questions</li>
            <li><strong>Iterative retrieval:</strong> each sub-question is processed independently</li>
            <li><strong>Sufficiency verification:</strong> the system checks whether the information is sufficient</li>
            <li><strong>Supplementation:</strong> additional questions are generated as needed</li>
        </ul>
        <p>
            <strong>Expected benefit:</strong> better performance on questions that require multiple reasoning steps,
            such as comparative or inferential questions.
        </p>

        <div class="subsubsection-title">5.1.2 Hybrid Retrieval Collaboration</div>
        <p>
            Using three searchers simultaneously provides complementary advantages:
        </p>
        <ul>
            <li><strong>Sparse searcher:</strong> precise for proper names, dates, and exact phrases</li>
            <li><strong>Dense searcher:</strong> good for semantic understanding and conceptual questions</li>
            <li><strong>Web searcher:</strong> access to current information and broad coverage</li>
        </ul>
        <p>
            <strong>Expected benefit:</strong> higher retrieval coverage by combining complementary strengths, reducing
            "not found" cases that occur in single-source methods.
        </p>

        <div class="subsubsection-title">5.1.3 Discourse Awareness</div>
        <p>
            Two-level discourse modeling provides several theoretical advantages:
        </p>
        <ul>
            <li><strong>RST trees:</strong> identify nucleus information and reduce the "lost in the middle" effect</li>
            <li><strong>Rhetorical graph:</strong> detect contradictions and prevent merging conflicting information</li>
            <li><strong>Planning:</strong> produce coherent responses (responses that stay logically connected and easy to follow) with logical structure. <em>Example:</em> "The battery is low, so the laptop shuts down; therefore I plug it in."</li>
        </ul>
        <p>
            <strong>Expected benefit:</strong> higher accuracy in prioritizing information(<em>Example:</em> in a long paragraph, it keeps the main point (e.g., "The phone is water‑resistant"
            and drops extra details.)), better handling of contradictions,
            and more coherent responses. 
        </p>

        <div class="subsection-title">5.2 Conceptual Case Study</div>
        
        <div class="definition">
            <strong>Example: "Are all TVs smart?"</strong>
            
            <p><strong>Standard RAG (expected behavior):</strong></p>
            <p>It might retrieve a definition of "smart TV" and answer "yes" without considering the differences between old and new models.</p>
            
            <p><strong>humanoid RAG (expected behavior):</strong></p>
            <ol>
                <li><strong>Retrieval:</strong> multiple sources including category and release-time information
                    <ul>
                        <li>Section 1: "A smart TV can connect to the Internet and run apps"</li>
                        <li>Section 2: "Old TVs only have video input and are not smart"</li>
                        <li>Section 3: "Today many new TVs are smart"</li>
                    </ul>
                </li>
                <li><strong>Discourse analysis:</strong> the distinction between old and new models is identified</li>
                <li><strong>Planning:</strong> explain category differences, then summarize</li>
                <li><strong>Generation:</strong> "No, not all TVs are smart. Old TVs are simple, but many new TVs are smart."</li>
            </ol>
            
            <p><strong>Key insight:</strong> the system distinguishes between different product categories and provides a more accurate answer.</p>
        </div>

        <div class="subsection-title">5.3 Expected Component Contributions</div>
        
        <div class="subsubsection-title">5.3.1 High-Level Planner Operations</div>
        <ul>
            <li><strong>Decomposition:</strong> critical for breaking complex questions; expected to be most impactful for multi-step questions</li>
            <li><strong>Summarization:</strong> compresses information while preserving key facts</li>
            <li><strong>Verification:</strong> ensures that retrieved information is sufficient to answer the question</li>
            <li><strong>Supplementation:</strong> fills information gaps by generating additional questions</li>
        </ul>

        <div class="subsubsection-title">5.3.2 Sparse Searcher Refinements</div>
        <ul>
            <li><strong>Extend:</strong> adds related keywords to expand search</li>
            <li><strong>Filter:</strong> reduces noise by removing irrelevant phrases</li>
            <li><strong>Emphasize:</strong> increases the importance of key phrases</li>
            <li><strong>Expected benefit:</strong> more accurate entity retrieval compared to plain BM25</li>
        </ul>

        <div class="subsubsection-title">5.3.3 Discourse Components</div>
        <ul>
            <li><strong>RST trees:</strong> help prioritize important sentences, reducing the "lost in the middle" effect</li>
            <li><strong>Rhetorical graphs:</strong> enable detection and resolution of contradictions</li>
            <li><strong>Planning scheme:</strong> provides structured guidance for coherent generation</li>
        </ul>

    <div class="section">
        <div class="section-title">6. Limitations and Considerations</div>

        <div class="subsection-title">6.1 Evaluation Limitations</div>
        
        <div class="warning">
            <strong>⚠️ Core Research Limitations:</strong>
            <ul>
                <li><strong>Model:</strong> only tested with LLaMA 7B</li>
                <li><strong>Data:</strong> evaluated only on private data</li>
                <li><strong>Benchmarks:</strong> not evaluated on public benchmark datasets (PopQA, NQ, TriviaQA, HotpotQA, 2WikiMultiHopQA)</li>
                <li><strong>Comparison:</strong> no statistical comparison with baseline systems</li>
                <li><strong>Ablations:</strong> no formal ablation studies to measure component contributions (ablation = remove one part to see how much it matters)</li>
            </ul>
        </div>

        <div class="subsection-title">6.2 Technical Limitations</div>
        
        <div class="subsubsection-title">6.2.1 Computational Complexity</div>
        <p>
            humanoid RAG incurs higher computational cost than simple RAG due to multiple LLM calls (for decomposition, summarization, verification, supplementation, and generation)
            and discourse processing. This may increase response time and cost in production environments.
        </p>

        <div class="subsubsection-title">6.2.2 Discourse Parsing Accuracy</div>
        <p>
            Building RST trees and rhetorical graphs depends on the accuracy of the discourse parser. Errors in identifying rhetorical relations can lead to incorrect responses.
            Parsing accuracy varies by domain and text quality.
        </p>
        <p><em>Example:</em> in “It is raining, so I take an umbrella,” if the parser flips it, it may think “I take an umbrella, so it is raining,” which is wrong. This kind of mistake happens more with messy text (missing punctuation), very long sentences, informal chat, or domain‑specific jargon (special terms used only in one field, like medicine or law; e.g., hypertension, biopsy, injunction, tort).</p>
        <p><em>Technical fix:</em> use a parser trained on the same kind of text, run a second parser to confirm, add simple rules to stop flipped cause/result (cause vs. result, e.g., “It rained, so the ground is wet” not “The ground is wet, so it rained”), and if the model is unsure, just keep the main sentence. <em>Example (medical, simple):</em> “The patient has a fever, so we give medicine.” If parsers disagree about the cause, keep only “The patient has a fever” and avoid claiming what caused it.</p>

        <div class="subsubsection-title">6.2.3 Dependence on Prompt Engineering</div>
        <p>
            System performance heavily depends on the quality of prompts designed for each operation.
            Poor prompts can lead to incorrect decomposition, insufficient summaries, or failed verification.
        </p>
        <p><em>Plain-language example:</em> This system is like a multi-step chef who needs a clear instruction at each step. If you only say “make something,” the chef might skip the salt or mix up the steps. But if you say “first chop the ingredients, then saute for 3 minutes, and add salt at the end,” the result is more reliable.</p>
        <p><em>Bad prompt:</em> <code>Process this text and give me the result.</code> (too vague; it does not say whether to decompose, summarize, or verify)</p>
        <p><em>Good prompt:</em> <code>Decompose the text into 3 separate sub-questions. For each sub-question: 1) provide a 2-sentence evidence summary, 2) if evidence is insufficient, say “insufficient.” Return a numbered list.</code></p>

        <div class="subsubsection-title">6.2.4 Retrieval Coverage</div>
        <p>
            Despite using three searchers, some relevant information may still be missed,
            especially if it does not exist in the local dataset or is phrased in a way that does not match the query.
        </p>
        <p><em>Plain-language example:</em> You ask three friends to find your phone charger in the house. One looks in the living room, one in the kitchen, one in your backpack. If the charger is actually in the car or labeled “USB‑C cable” instead of “charger,” they might still miss it.</p>

        <div class="subsection-title">6.3 Practical Limitations</div>
        
        <div class="subsubsection-title">6.3.1 Latency</div>
        <p>
            The iterative nature of the system (decompose-retrieve-summarize-verify-supplement loop) can lead to significant latency,
            especially for complex questions that require multiple iterations.
        </p>
        <p><em>Plain-language example:</em> It’s like a person who has to do a task in several rounds: first break the question into parts, then go search for each part, then summarize what they found, then double-check it, and only after that give a final answer. For a simple question, this feels quick, but for a complex one it’s like taking multiple trips back and forth, so the wait time grows.</p>

        <div class="subsubsection-title">6.3.2 Cost</div>
        <p>
            Multiple LLM calls and retrieval operations can increase cost, especially when using commercial APIs.
            For high-volume applications, this cost can be significant.
        </p>
        <p><em>Plain-language example:</em> If you call a paid service several times for each question, the bill adds up quickly. But you can use a local model runner like <em>Ollama</em> (a tool that runs models on your own machine) instead of commercial APIs, and choose a small multi‑billion‑parameter model that fits your computer’s capacity. Try a few different models and see which one gives the best answers for your data. That makes each request much cheaper, as long as you balance response time and quality with your hardware limits.</p>

        <div class="subsection-title">6.4 Future Work</div>
        <p>Future work may include:</p>
        <ul>
            <li>Evaluation on public benchmark datasets</li>
            <li>Ablation studies to precisely measure each component's contribution</li>
            <li>Experiments with different language models (GPT-4, Claude, Mistral, etc.)</li>
            <li>Optimization to reduce latency and cost</li>
            <li>Improving discourse parsing accuracy</li>
            <li>Developing caching techniques to reduce repeated LLM calls</li>
        </ul>
        <p><em>How to do it (short answers):</em></p>
        <ul>
            <li><strong>Public benchmarks:</strong> pick standard datasets, build a fixed evaluation script, and report metrics like EM/F1 (Exact Match and F1 score) so results are comparable. These are used to measure answer correctness in a consistent way: EM checks if the prediction is exactly the same as the gold answer, while F1 measures partial overlap between words. To use them for testing, run your model on a benchmark, compare each output to the gold answer with EM and F1, then average the scores across all questions. <em>Plain‑language example:</em> If the correct answer is “Paris,” EM is 1 only when the model says exactly “Paris.” If it says “Paris, France,” EM becomes 0, but F1 can still be high because most of the words match.</li>
            <li><strong>Ablations:</strong> remove or disable one component at a time (e.g., discourse parsing) while keeping everything else fixed, then compare quality, latency, and cost.</li>
            <li><strong>Different LMs:</strong> swap only the model (keep prompts and data the same) and compare answer quality, speed, and price.</li>
            <li><strong>Latency/cost:</strong> reduce iterations, batch calls, run smaller or quantized models, and parallelize retrieval where possible.</li>
            <li><strong>Discourse parsing:</strong> use a parser trained on your domain or fine‑tune it, then validate with manual samples.</li>
            <li><strong>Caching:</strong> caching means “save an answer so you don’t have to do the work again.” In simple terms: before you solve a question, you first check if you already solved the same one; if yes, you reuse it. If not, you solve it and save the result for next time. <em>Does it take too much space?</em> It can, so keep only recent or popular items and delete old ones after a time limit (TTL). <em>Example:</em> If many users ask “What is RAG?”, the system answers once, saves it, and then returns the saved answer instantly for the next users.</li>
        </ul>
    </div>

    <div class="section">
        <div class="section-title">7. Implementation Guide</div>

        <div class="subsection-title">7.1 Proposed Tech Stack</div>
        
        <div class="table">
            <table>
                <thead>
                    <tr>
                        <th>Component</th>
                        <th>Suggested Tools</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Language model (planner)</strong></td>
                        <td>LLaMA 7B, GPT-3.5-turbo, Mistral-7B, Claude</td>
                    </tr>
                    <tr>
                        <td><strong>Sparse searcher</strong></td>
                        <td>Elasticsearch, BM25 (rank_bm25), Apache Solr</td>
                    </tr>
                    <tr>
                        <td><strong>Dense searcher</strong></td>
                        <td>FAISS, Pinecone, Weaviate, ChromaDB, sentence-transformers</td>
                    </tr>
                    <tr>
                        <td><strong>Web searcher</strong></td>
                        <td>DuckDuckGo Instant Answer API (free)</td>
                    </tr>
                    <tr>
                        <td><strong>Discourse parsing</strong></td>
                        <td>Custom transformer models, NLI models (RoBERTa-NLI)</td>
                    </tr>
                    <tr>
                        <td><strong>Framework</strong></td>
                        <td>LangChain, LlamaIndex, Haystack</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="subsection-title">7.2 Proposed Architecture</div>
        
        <div class="algorithm">
            <strong>Algorithm 2: humanoid RAG Pipeline</strong>
            <pre>
class HumanoidRAG:
    def __init__(self, llm, sparse, dense, web):
        self.planner = HighLevelPlanner(llm)
        self.sparse_searcher = SparseSearcher(sparse)
        self.dense_searcher = DenseSearcher(dense)
        self.web_searcher = WebSearcher(web)
        self.discourse_parser = DiscourseParser()
    
    def answer(self, query):
        # Step 1: Decompose
        sub_queries = self.planner.decompose(query)
        
        # Step 2: Retrieve
        all_chunks = []
        for sq in sub_queries:
            sparse_chunks = self.sparse_searcher.search(sq)
            dense_chunks = self.dense_searcher.search(sq)
            web_chunks = self.web_searcher.search(sq)
            all_chunks.extend([sparse_chunks, dense_chunks, web_chunks])
        
        # Step 3: Discourse modeling
        rst_trees = self.discourse_parser.build_rst_trees(all_chunks)
        rhetorical_graph = self.discourse_parser.build_graph(all_chunks)
        
        # Step 4: Summarize
        summaries = []
        for sq, chunks in zip(sub_queries, grouped_chunks):
            nucleus_info = extract_nucleus(chunks, rst_trees)
            summary = self.planner.summarize(sq, nucleus_info)
            summaries.append(summary)
        
        # Step 5: Verify and supplement
        while not self.planner.verify(query, summaries):
            new_query = self.planner.supplement(query, summaries)
            # repeat retrieval and summarization...
        
        # Step 6: Plan and generate
        plan = self.discourse_parser.create_plan(rhetorical_graph)
        answer = self.planner.generate(query, summaries, plan)
        
        return answer
            </pre>
        </div>

        <div class="subsection-title">7.3 Implementation Tips</div>
        <ul>
            <li><strong>Start simple:</strong> implement a basic RAG first, then gradually add hierarchical planning and discourse modeling</li>
            <li><strong>Caching:</strong> cache retrieval results and LLM responses to reduce cost and latency</li>
            <li><strong>Parallel processing:</strong> run the three searchers in parallel</li>
            <li><strong>Error handling:</strong> provide fallback solutions when discourse parsing or verification fails</li>
            <li><strong>Monitoring:</strong> log the performance of each searcher and component for optimization</li>
        </ul>
    </div>

    <div class="section">
        <div class="section-title">8. Conclusion</div>
        
        <p>
            We introduced humanoid RAG, a novel framework that combines hierarchical question planning with discourse modeling to address the challenges of traditional RAG systems on complex questions.
            Our two-level architecture—with a high-level planner for question decomposition and sufficiency verification, and low-level searchers for hybrid retrieval—provides a structured approach to multi-step reasoning.
        </p>
        
        <p>
            Our two-level discourse modeling—using intra-section RST trees to identify nucleus information and an inter-section rhetorical graph to model relations such as contradiction—enables more accurate and coherent responses.
            This discourse structure guides response planning and ensures that contradictions are handled correctly and information is presented in a logical order.
        </p>
        
        <div class="warning">
            <strong>⚠️ Important Reminder:</strong>
            <p>
                This research is a conceptual and theoretical study tested only on private data with the LLaMA 7B model.
                Comprehensive evaluation on public benchmarks and ablation studies are required to validate the claimed benefits.
            </p>
        </div>
        
        <p>
            Future work includes evaluation on standard benchmarks, experiments with different language models, and optimization for production environments.
            We hope that humanoid RAG can serve as a foundation for further research on discourse-aware RAG systems.
        </p>
    </div>

    <div class="section">
        <div class="section-title">Acknowledgments</div>
        <p>
            This research builds on pioneering work in Retrieval-Augmented Generation (RAG) and discourse modeling.
            We thank the open-source community and the authors of the LevelRAG and DiscoRAG papers for their inspiring work.
        </p>
    </div>

    <div class="references">
        <div class="section-title">References</div>
        
        <div class="reference-item">
            [1] T. B. Brown et al., "Language Models are Few-Shot Learners," <em>NeurIPS</em>, 2020.
        </div>
        
        <div class="reference-item">
            [2] H. Touvron et al., "LLaMA: Open and Efficient Foundation Language Models," 
            <em>arXiv:2302.13971</em>, 2023.
        </div>
        
        <div class="reference-item">
            [3] P. Lewis et al., "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks," 
            <em>NeurIPS</em>, 2020.
        </div>
        
        <div class="reference-item">
            [4] A. Asai et al., "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection," 
            <em>ICLR</em>, 2024.
        </div>
        
        <div class="reference-item">
            [5] H. Trivedi et al., "Interleaving Retrieval with Chain-of-Thought Reasoning," 
            <em>arXiv:2212.10509</em>, 2023.
        </div>
        
        <div class="reference-item">
            [6] W. C. Mann and S. A. Thompson, "Rhetorical Structure Theory: Toward a Functional Theory of Text Organization," 
            <em>Text</em>, vol. 8, no. 3, pp. 243-281, 1988.
        </div>
        
        <div class="reference-item">
            [7] Z. Chen et al., "MindSearch: Mimicking Human Minds Elicits Deep AI Searcher," 
            <em>arXiv:2407.20183</em>, 2024.
        </div>
        
        <div class="reference-item">
            [8] L. Gao et al., "Precise Zero-Shot Dense Retrieval without Relevance Labels," 
            <em>arXiv:2212.10496</em>, 2022.
        </div>
        
        <div class="reference-item">
            [9] L. Wang et al., "Query2doc: Query Expansion with Large Language Models," 
            <em>arXiv:2303.07678</em>, 2023.
        </div>
        
        <div class="reference-item">
            [10] Y. Gao et al., "Retrieval-Augmented Generation for Large Language Models: A Survey," 
            <em>arXiv:2312.10997</em>, 2024.
        </div>
    </div>

    <div class="section" style="margin-top: 60px; padding-top: 20px; border-top: 2px solid #333;">
        <div class="section-title">Author Information</div>
        <div class="warning">
            <strong>Author:</strong> Fardin Ibrahimi<br>
            <strong>Degree:</strong> B.Sc. in Computer Science<br>
            <strong>Status:</strong> Independent research - no organizational affiliation<br>
            <strong>Funding:</strong> This research was not funded by any organization or institution
        </div>
        </div>
    </div>

    <script>
        // Convert numbers to English
        function toEnglishNumber(num) {
            return num.toString();
        }
        
        // Calculate page number based on scroll
        window.addEventListener('scroll', function() {
            var pageHeight = 1122; // height of an A4 page (px)
            var scrollPosition = window.scrollY;
            var pageNumber = Math.floor(scrollPosition / pageHeight) + 1;
            
            var pageNumberElement = document.getElementById('pageNumber');
            if (pageNumberElement) {
                // Display in "Page X of Y" format
                var totalHeight = document.body.scrollHeight;
                var totalPages = Math.ceil(totalHeight / pageHeight);
                pageNumberElement.textContent = 'Page ' + toEnglishNumber(pageNumber) + ' of ' + toEnglishNumber(totalPages);
            }
        });
        
        // Initial page number setup
        window.addEventListener('load', function() {
            var pageHeight = 1122;
            var totalHeight = document.body.scrollHeight;
            var totalPages = Math.ceil(totalHeight / pageHeight);
            
            var pageNumberElement = document.getElementById('pageNumber');
            if (pageNumberElement) {
                pageNumberElement.textContent = 'Page ' + toEnglishNumber(1) + ' of ' + toEnglishNumber(totalPages);
            }
            
            console.log('Total pages: ' + totalPages);
        });
        
        function mmToPx(mm) {
            var probe = document.createElement('div');
            probe.style.position = 'absolute';
            probe.style.visibility = 'hidden';
            probe.style.height = mm + 'mm';
            probe.style.width = '1px';
            document.body.appendChild(probe);
            var px = probe.getBoundingClientRect().height;
            document.body.removeChild(probe);
            return px;
        }

        function clearPrintNumbers() {
            var oldNumbers = document.querySelectorAll('.print-page-number.dynamic');
            oldNumbers.forEach(function(el) { el.remove(); });
        }

        function renderPrintNumbers() {
            clearPrintNumbers();

            // A4 page height in mm
            var pageHeightPx = mmToPx(297);
            if (!pageHeightPx || !isFinite(pageHeightPx)) {
                return;
            }

            var totalHeight = Math.max(document.body.scrollHeight, document.documentElement.scrollHeight);
            var totalPages = Math.max(1, Math.ceil(totalHeight / pageHeightPx));
            var bottomOffset = mmToPx(10); // keep above bottom margin

            for (var pageNum = 1; pageNum <= totalPages; pageNum++) {
                var pageNumberDiv = document.createElement('div');
                pageNumberDiv.className = 'print-page-number dynamic';
                pageNumberDiv.style.position = 'absolute';
                pageNumberDiv.style.left = '50%';
                pageNumberDiv.style.transform = 'translateX(-50%)';
                pageNumberDiv.style.top = (pageNum * pageHeightPx - bottomOffset) + 'px';
                pageNumberDiv.textContent = 'Page ' + pageNum + ' of ' + totalPages;
                document.body.appendChild(pageNumberDiv);
            }
        }

        function setupPrintListeners() {
            window.addEventListener('beforeprint', function() {
                setTimeout(renderPrintNumbers, 0);
            });
            window.addEventListener('afterprint', clearPrintNumbers);
            window.onbeforeprint = function() { setTimeout(renderPrintNumbers, 0); };
            window.onafterprint = clearPrintNumbers;

            if (window.matchMedia) {
                var mediaQueryList = window.matchMedia('print');
                if (mediaQueryList.addEventListener) {
                    mediaQueryList.addEventListener('change', function(e) {
                        if (e.matches) {
                            setTimeout(renderPrintNumbers, 0);
                        } else {
                            clearPrintNumbers();
                        }
                    });
                } else if (mediaQueryList.addListener) {
                    mediaQueryList.addListener(function(mql) {
                        if (mql.matches) {
                            setTimeout(renderPrintNumbers, 0);
                        } else {
                            clearPrintNumbers();
                        }
                    });
                }
            }
        }

        setupPrintListeners();
    </script>

</body>
</html>
